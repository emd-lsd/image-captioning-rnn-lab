# Лабораторная работа: Генерация подписей к изображениям (RNN/LSTM)

Этот проект реализует модель для генерации подписей к изображениям с использованием архитектуры CNN+RNN на датасете COCO Captions.

## Структура проекта
- `/notebooks/image_captioning.ipynb`: Jupyter Notebook со всем кодом, анализом и выводами.
- `coco_utils.py`: Вспомогательный скрипт для загрузки данных.
- `requirements.txt`: Список необходимых библиотек.

## Настройка и запуск

### 1. Загрузка данных
**Внимание:** Датасет не включен в репозиторий из-за большого размера.
1.  Скачайте архив с данными (~1 Гб) по ссылке: [http://labcolor.space/coco_captioning.zip](http://labcolor.space/coco_captioning.zip)
2.  Создайте в корне проекта папку `datasets`.
3.  Внутри нее создайте папку `coco_captioning`.
4.  Распакуйте содержимое скачанного архива в папку `datasets/coco_captioning/`.

### 2. Установка зависимостей
Убедитесь, что у вас установлен Python 3.9+ и Git. Откройте терминал в корневой папке проекта и выполните:
```bash
pip install -r requirements.txt


# Выводы по лабораторной работе

В ходе данной лабораторной работы была реализована модель для генерации подписей к изображениям (Image Captioning) на основе архитектуры CNN+RNN.

### 1. Реализованные архитектуры
Были реализованы и сравнены две архитектуры для объединения признаков изображения и текстовых данных, описанные в статье "Where to put the Image in an Image Caption Generator":
1.  **(a) Init-inject:** Вектор признаков изображения используется для инициализации начального скрытого состояния LSTM-декодера.
2.  **(b) Pre-inject:** Вектор признаков изображения подается как самый первый элемент последовательности на вход LSTM.

### 2. Процесс обучения
Обе модели обучались в одинаковых условиях для честного сравнения: 4 эпохи на полном обучающем датасете COCO с использованием GPU. В процессе обучения отслеживалась функция потерь (Cross-Entropy Loss) на обучающей и валидационной выборках. Было экспериментально установлено, что обучение более 4-х эпох приводит к **переобучению** (росту `Val Loss`).

### 3. Сравнение моделей и результаты
Сравнение проводилось как визуально, так и с помощью стандартной метрики **BLEU**.

-   **Визуальная оценка:** Модель `Init-inject` генерировала более длинные, осмысленные и релевантные изображениям подписи. Модель `Pre-inject` часто генерировала лишь 2-3 слова и обрывалась на токенах `<UNK>`.
-   **Оценка BLEU:** Численные результаты полностью подтвердили визуальные наблюдения. Модель `Init-inject` показала значительно лучшие результаты по всем метрикам BLEU, превосходя вторую модель в 2-3 раза.

| Метрика | Init-inject | Pre-inject |
| :--- | :---: | :---: |
| **BLEU-1** | **0.2927** | 0.1453 |
| **BLEU-2** | **0.1464** | 0.0398 |
| **BLEU-3** | **0.0881** | 0.0243 |
| **BLEU-4** | **0.0619** | 0.0209 |

### Итоговое заключение
Экспериментально установлено, что для данной задачи архитектура **Init-inject** является значительно более эффективной. Она быстрее сходится (показывает меньший `Loss`) и генерирует более качественные и полные подписи, что подтверждается как визуальной оценкой, так и метрикой BLEU.
